{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(layer_sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        act_func = activation if (i < len(layer_sizes)-2) else output_activation\n",
    "        layers += [nn.Linear(layer_sizes[i], layer_sizes[i+1]), act_func()]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp([10, 32, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env = \"CartPole-v0\"\n",
    "\n",
    "env = gym.make(gym_env)\n",
    "\n",
    "assert isinstance(env.observation_space, gym.spaces.Box), \"This environment only works for environment with continuous state spaces\"\n",
    "assert isinstance(env.action_space, gym.spaces.Discrete), \"This environment only works for environment with discrete action spaces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZES = [32]\n",
    "\n",
    "logits_net = mlp([obs_dim]+HIDDEN_SIZES+[act_dim])\n",
    "logits_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(obs):\n",
    "    logits  = logits_net(obs)\n",
    "    return Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs):\n",
    "    return get_policy(obs).sample().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(obs, acts, weights):\n",
    "    logp = get_policy(obs).log_prob(acts)\n",
    "    return -(logp * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: Categorical(logits: torch.Size([100, 2]))\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.7274, -0.6375, -0.6431, -0.4836, -0.5551, -0.5950, -0.5674, -0.8721,\n",
       "        -1.0282, -0.9089, -0.5469, -0.6654, -0.6133, -0.8735, -0.8167, -0.6684,\n",
       "        -0.6578, -0.8105, -0.6609, -0.7503, -0.8359, -0.7647, -0.6769, -0.6627,\n",
       "        -0.8990, -0.5357, -0.6252, -0.9206, -0.5655, -0.8761, -0.4550, -0.7382,\n",
       "        -0.5013, -0.5299, -0.7774, -0.6554, -0.5443, -0.8650, -0.7305, -0.7554,\n",
       "        -0.8329, -0.7287, -0.8471, -0.5098, -0.5916, -0.5357, -0.9608, -0.8672,\n",
       "        -0.8355, -0.8727, -0.6262, -0.6127, -0.5338, -0.9311, -0.5907, -0.8618,\n",
       "        -0.5818, -0.5532, -0.5281, -0.9945, -0.8214, -0.4418, -0.7916, -0.6644,\n",
       "        -0.9708, -0.7182, -0.7832, -0.8636, -1.0214, -0.4789, -0.5438, -0.7430,\n",
       "        -0.7120, -0.6144, -0.6762, -0.5949, -0.6695, -0.5573, -0.7534, -0.8118,\n",
       "        -0.6147, -1.0073, -1.0187, -1.0009, -0.5821, -0.6101, -0.9304, -0.6295,\n",
       "        -0.5807, -0.5537, -0.7836, -0.7905, -0.4483, -0.7675, -0.8361, -0.8706,\n",
       "        -0.6805, -0.5934, -0.6450, -0.5631], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = torch.randn(100, 4) # batch, obs_dim\n",
    "policy = get_policy(obs) # Categorical Policy - a distribution is returned\n",
    "\n",
    "print(\"Policy:\", policy)\n",
    "print(policy.sample())\n",
    "acts = torch.randint(0, 2, [100])\n",
    "policy.log_prob(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(env, optimizer, batch_size, render=True):\n",
    "    batch_obs = []\n",
    "    batch_acts = []\n",
    "    batch_weights = []\n",
    "    batch_rets = []\n",
    "    batch_lens = []\n",
    "    \n",
    "    # get initial observation from starting distribution\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_rews = []\n",
    "    \n",
    "    # Render the first episode of the epoch\n",
    "    finish_rendering_this_epoch = False\n",
    "    \n",
    "    while True:\n",
    "        # rendering\n",
    "        if (not finish_rendering_this_epoch) and render:\n",
    "            env.render()\n",
    "            # plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "            # display.display(plt.gcf())\n",
    "            # display.clear_output(wait=True)\n",
    "        \n",
    "        # save the current observation\n",
    "        batch_obs.append(obs.copy())\n",
    "        \n",
    "        # get action for the current observation\n",
    "        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "        \n",
    "        # save the action and reward\n",
    "        ep_rews.append(rew)\n",
    "        batch_acts.append(act)\n",
    "        \n",
    "        if done:\n",
    "            # record info about the episode\n",
    "            ep_ret = sum(ep_rews)\n",
    "            ep_len = len(ep_rews)\n",
    "            \n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            batch_weights += [ep_ret] * ep_len # the weights are the returns for each episode, broadcasted to support\n",
    "                                                # the operation in function compute_loss\n",
    "            \n",
    "            #reset the environment\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            ep_rews = []\n",
    "            \n",
    "            # won't render again after first episode in the epoch\n",
    "            finish_rendering_this_epoch = True\n",
    "            \n",
    "            # end experience loop if we have enough of it\n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "        \n",
    "    # perform a single update step\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32), \n",
    "                              acts=torch.as_tensor(batch_acts, dtype=torch.float32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32))\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    env.close()\n",
    "\n",
    "    return batch_loss.item(), batch_rets, batch_lens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "\n",
    "optimizer = torch.optim.Adam(logits_net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 18.80 Average Return: 21.6 Average Steps: 21.6\n",
      "Epoch [2/50], Loss: 18.02 Average Return: 24.1 Average Steps: 24.1\n",
      "Epoch [3/50], Loss: 38.26 Average Return: 58.5 Average Steps: 58.5\n",
      "Epoch [4/50], Loss: 16.72 Average Return: 30.2 Average Steps: 30.2\n",
      "Epoch [5/50], Loss: 14.36 Average Return: 28.9 Average Steps: 28.9\n",
      "Epoch [6/50], Loss: 15.51 Average Return: 34.0 Average Steps: 34.0\n",
      "Epoch [7/50], Loss: 18.60 Average Return: 45.0 Average Steps: 45.0\n",
      "Epoch [8/50], Loss: 27.18 Average Return: 69.6 Average Steps: 69.6\n",
      "Epoch [9/50], Loss: 33.27 Average Return: 101.3 Average Steps: 101.3\n",
      "Epoch [10/50], Loss: 25.82 Average Return: 92.0 Average Steps: 92.0\n",
      "Epoch [11/50], Loss: 24.51 Average Return: 102.1 Average Steps: 102.1\n",
      "Epoch [12/50], Loss: 22.96 Average Return: 109.5 Average Steps: 109.5\n",
      "Epoch [13/50], Loss: 21.48 Average Return: 113.4 Average Steps: 113.4\n",
      "Epoch [14/50], Loss: 23.43 Average Return: 127.9 Average Steps: 127.9\n",
      "Epoch [15/50], Loss: 30.56 Average Return: 174.7 Average Steps: 174.7\n",
      "Epoch [16/50], Loss: 30.10 Average Return: 186.9 Average Steps: 186.9\n",
      "Epoch [17/50], Loss: 30.74 Average Return: 199.5 Average Steps: 199.5\n",
      "Epoch [18/50], Loss: 25.44 Average Return: 182.5 Average Steps: 182.5\n",
      "Epoch [19/50], Loss: 22.09 Average Return: 181.7 Average Steps: 181.7\n",
      "Epoch [20/50], Loss: 22.01 Average Return: 193.2 Average Steps: 193.2\n",
      "Epoch [21/50], Loss: 24.45 Average Return: 200.0 Average Steps: 200.0\n",
      "Epoch [22/50], Loss: 21.02 Average Return: 200.0 Average Steps: 200.0\n",
      "Epoch [23/50], Loss: 20.50 Average Return: 183.8 Average Steps: 183.8\n",
      "Epoch [24/50], Loss: 17.38 Average Return: 158.6 Average Steps: 158.6\n",
      "Epoch [25/50], Loss: 16.14 Average Return: 145.5 Average Steps: 145.5\n",
      "Epoch [26/50], Loss: 13.45 Average Return: 145.8 Average Steps: 145.8\n",
      "Epoch [27/50], Loss: 13.01 Average Return: 136.8 Average Steps: 136.8\n",
      "Epoch [28/50], Loss: 12.07 Average Return: 142.4 Average Steps: 142.4\n",
      "Epoch [29/50], Loss: 11.65 Average Return: 146.2 Average Steps: 146.2\n",
      "Epoch [30/50], Loss: 12.53 Average Return: 144.7 Average Steps: 144.7\n",
      "Epoch [31/50], Loss: 12.10 Average Return: 134.9 Average Steps: 134.9\n",
      "Epoch [32/50], Loss: 9.60 Average Return: 121.4 Average Steps: 121.4\n",
      "Epoch [33/50], Loss: 10.84 Average Return: 115.2 Average Steps: 115.2\n",
      "Epoch [34/50], Loss: 9.29 Average Return: 113.0 Average Steps: 113.0\n",
      "Epoch [35/50], Loss: 11.21 Average Return: 117.7 Average Steps: 117.7\n",
      "Epoch [36/50], Loss: 9.49 Average Return: 102.8 Average Steps: 102.8\n",
      "Epoch [37/50], Loss: 5.93 Average Return: 88.3 Average Steps: 88.3\n",
      "Epoch [38/50], Loss: 5.53 Average Return: 86.5 Average Steps: 86.5\n",
      "Epoch [39/50], Loss: 4.84 Average Return: 80.7 Average Steps: 80.7\n",
      "Epoch [40/50], Loss: 5.66 Average Return: 78.5 Average Steps: 78.5\n",
      "Epoch [41/50], Loss: 3.74 Average Return: 70.2 Average Steps: 70.2\n",
      "Epoch [42/50], Loss: 4.23 Average Return: 70.4 Average Steps: 70.4\n",
      "Epoch [43/50], Loss: 4.95 Average Return: 74.2 Average Steps: 74.2\n",
      "Epoch [44/50], Loss: 3.74 Average Return: 64.6 Average Steps: 64.6\n",
      "Epoch [45/50], Loss: 6.15 Average Return: 73.7 Average Steps: 73.7\n",
      "Epoch [46/50], Loss: 3.74 Average Return: 67.9 Average Steps: 67.9\n",
      "Epoch [47/50], Loss: 4.91 Average Return: 69.3 Average Steps: 69.3\n",
      "Epoch [48/50], Loss: 8.53 Average Return: 78.6 Average Steps: 78.6\n",
      "Epoch [49/50], Loss: 12.83 Average Return: 91.9 Average Steps: 91.9\n",
      "Epoch [50/50], Loss: 9.02 Average Return: 83.4 Average Steps: 83.4\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZES = 5_000\n",
    "EPOCHS = 50\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_loss, batch_rets, batch_lens = train_one_epoch(env, optimizer, BATCH_SIZES, render=True)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {batch_loss:.2f} Average Return: {np.mean(batch_rets):.1f} Average Steps: {np.mean(batch_lens):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7dfebdb0d38b25502d3e903ebcd5598394fdc0b22308289bf943c5905d42435"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
