{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(layer_sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        act_func = activation if (i < len(layer_sizes)-2) else output_activation\n",
    "        layers += [nn.Linear(layer_sizes[i], layer_sizes[i+1]), act_func()]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp([10, 32, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "gym_env = \"CartPole-v0\"\n",
    "\n",
    "env = gym.make(gym_env)\n",
    "\n",
    "assert isinstance(env.observation_space, gym.spaces.Box), \"This environment only works for environment with continuous state spaces\"\n",
    "assert isinstance(env.action_space, gym.spaces.Discrete), \"This environment only works for environment with discrete action spaces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "obs_dim, act_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZES = [32]\n",
    "\n",
    "logits_net = mlp([obs_dim]+HIDDEN_SIZES+[act_dim])\n",
    "logits_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(obs):\n",
    "    logits  = logits_net(obs)\n",
    "    return Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs):\n",
    "    return get_policy(obs).sample().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(obs, acts, weights):\n",
    "    logp = get_policy(obs).log_prob(acts)\n",
    "    return -(logp * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: Categorical(logits: torch.Size([100, 2]))\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.8048, -0.7958, -0.6597, -0.6978, -0.6389, -0.5997, -0.4492, -0.4949,\n",
       "        -0.6686, -1.0140, -0.8576, -0.7843, -0.5691, -0.6071, -0.5967, -0.6030,\n",
       "        -0.5098, -0.9116, -0.6056, -0.6143, -0.4514, -0.5895, -0.6495, -0.8055,\n",
       "        -0.5390, -0.9810, -0.9446, -0.4827, -0.6688, -0.7687, -0.4493, -0.8314,\n",
       "        -0.7401, -0.8872, -0.7082, -0.7315, -0.7846, -0.5859, -0.7569, -0.6626,\n",
       "        -0.4447, -0.5974, -0.8226, -0.7267, -0.8007, -0.7033, -0.9083, -0.6631,\n",
       "        -0.7833, -0.5968, -0.7593, -0.6720, -0.7442, -0.8379, -0.5073, -0.6141,\n",
       "        -0.8344, -0.9061, -0.7730, -0.6391, -0.5788, -0.4734, -0.7579, -0.4852,\n",
       "        -0.5709, -0.6351, -0.7606, -0.6004, -0.5593, -0.4578, -0.7238, -0.7585,\n",
       "        -0.6900, -0.4528, -0.6389, -0.7694, -0.5779, -0.8883, -0.5350, -0.7532,\n",
       "        -0.9592, -0.9247, -0.6001, -0.6218, -0.7731, -0.6907, -0.6701, -0.8141,\n",
       "        -0.5559, -0.6480, -0.6216, -0.5374, -0.7134, -0.7037, -0.4731, -0.8225,\n",
       "        -0.6979, -1.0025, -0.5567, -0.9433], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = torch.randn(100, obs_dim) # batch, obs_dim\n",
    "policy = get_policy(obs) # Categorical Policy - a distribution is returned\n",
    "\n",
    "print(\"Policy:\", policy)\n",
    "print(policy.sample())\n",
    "acts = torch.randint(0, 2, [100])\n",
    "policy.log_prob(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_to_go(ep_rews):\n",
    "    n = len(ep_rews)\n",
    "    rtgs = np.zeros_like(ep_rews) # same size as ep_rews\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = ep_rews[i] + (rtgs[i+1] if i + 1 < n else 0)\n",
    "    return rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(env, optimizer, batch_size, render=True):\n",
    "    batch_obs = []\n",
    "    batch_acts = []\n",
    "    batch_weights = []\n",
    "    batch_rets = []\n",
    "    batch_lens = []\n",
    "    \n",
    "    # get initial observation from starting distribution\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_rews = []\n",
    "    \n",
    "    # Render the first episode of the epoch\n",
    "    finish_rendering_this_epoch = False\n",
    "    \n",
    "    while True:\n",
    "        # rendering\n",
    "        if (not finish_rendering_this_epoch) and render:\n",
    "            env.render()\n",
    "            # plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "            # display.display(plt.gcf())\n",
    "            # display.clear_output(wait=True)\n",
    "        \n",
    "        # save the current observation\n",
    "        batch_obs.append(obs.copy())\n",
    "        \n",
    "        # get action for the current observation\n",
    "        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "        \n",
    "        # save the action and reward\n",
    "        ep_rews.append(rew)\n",
    "        batch_acts.append(act)\n",
    "        \n",
    "        if done:\n",
    "            # record info about the episode\n",
    "            ep_ret = sum(ep_rews)\n",
    "            ep_len = len(ep_rews)\n",
    "            \n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            # batch_weights += [ep_ret] * ep_len # the weights are the returns for each episode, broadcasted to support\n",
    "                                                # the operation in function compute_loss\n",
    "            # changes for the reward-to-go policy gradient\n",
    "            batch_weights += list(reward_to_go(ep_rews))\n",
    "            \n",
    "            #reset the environment\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            ep_rews = []\n",
    "            \n",
    "            # won't render again after first episode in the epoch\n",
    "            finish_rendering_this_epoch = True\n",
    "            \n",
    "            # end experience loop if we have enough of it\n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "        \n",
    "    # perform a single update step\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32), \n",
    "                              acts=torch.as_tensor(batch_acts, dtype=torch.float32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32))\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    env.close()\n",
    "\n",
    "    return batch_loss.item(), batch_rets, batch_lens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "\n",
    "optimizer = torch.optim.Adam(logits_net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 9.43 Average Return: 20.5 Average Steps: 20.5\n",
      "Epoch [2/50], Loss: 10.75 Average Return: 26.0 Average Steps: 26.0\n",
      "Epoch [3/50], Loss: 18.53 Average Return: 51.9 Average Steps: 51.9\n",
      "Epoch [4/50], Loss: 19.67 Average Return: 65.6 Average Steps: 65.6\n",
      "Epoch [5/50], Loss: 18.45 Average Return: 71.3 Average Steps: 71.3\n",
      "Epoch [6/50], Loss: 19.12 Average Return: 79.5 Average Steps: 79.5\n",
      "Epoch [7/50], Loss: 19.66 Average Return: 94.9 Average Steps: 94.9\n",
      "Epoch [8/50], Loss: 23.87 Average Return: 127.5 Average Steps: 127.5\n",
      "Epoch [9/50], Loss: 30.05 Average Return: 178.1 Average Steps: 178.1\n",
      "Epoch [10/50], Loss: 28.96 Average Return: 186.4 Average Steps: 186.4\n",
      "Epoch [11/50], Loss: 25.63 Average Return: 177.1 Average Steps: 177.1\n",
      "Epoch [12/50], Loss: 25.16 Average Return: 182.9 Average Steps: 182.9\n",
      "Epoch [13/50], Loss: 25.23 Average Return: 195.1 Average Steps: 195.1\n",
      "Epoch [14/50], Loss: 24.83 Average Return: 199.2 Average Steps: 199.2\n",
      "Epoch [15/50], Loss: 22.97 Average Return: 200.0 Average Steps: 200.0\n",
      "Epoch [16/50], Loss: 23.26 Average Return: 200.0 Average Steps: 200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-eeb49e16f41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training Loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{EPOCHS}], Loss: {batch_loss:.2f} Average Return: {np.mean(batch_rets):.1f} Average Steps: {np.mean(batch_lens):.1f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7d471e645b98>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(env, optimizer, batch_size, render)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# get action for the current observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f7affc1c12fd>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9d418594814d>\u001b[0m in \u001b[0;36mget_policy\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlogits\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlogits_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/spinningup/lib/python3.6/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZES = 5_000\n",
    "EPOCHS = 50\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_loss, batch_rets, batch_lens = train_one_epoch(env, optimizer, BATCH_SIZES, render=True)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {batch_loss:.2f} Average Return: {np.mean(batch_rets):.1f} Average Steps: {np.mean(batch_lens):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to adjust the functions get_policy, get_action, compute_loss to have parameter logits_net\n",
    "\n",
    "env2 = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "obs_dim = env2.observation_space.shape[0]\n",
    "act_dim = env2.action_space.n\n",
    "HIDDEN_SIZES2 = [32]\n",
    "\n",
    "logits_net2 = mlp([obs_dim] + HIDDEN_SIZES2 + [act_dim])\n",
    "\n",
    "# Training Loop\n",
    "# for epoch in range(EPOCHS):\n",
    "#     batch_loss, batch_rets, batch_lens = train_one_epoch(env2, optimizer, BATCH_SIZES, render=True)\n",
    "#     print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {batch_loss:.2f} Average Return: {np.mean(batch_rets):.1f} Average Steps: {np.mean(batch_lens):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7dfebdb0d38b25502d3e903ebcd5598394fdc0b22308289bf943c5905d42435"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
