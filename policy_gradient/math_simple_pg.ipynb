{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(layer_sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        act_func = activation if (i < len(layer_sizes)-2) else output_activation\n",
    "        layers += [nn.Linear(layer_sizes[i], layer_sizes[i+1]), act_func()]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp([10, 32, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "gym_env = \"CartPole-v0\"\n",
    "\n",
    "env = gym.make(gym_env)\n",
    "\n",
    "assert isinstance(env.observation_space, gym.spaces.Box), \"This environment only works for environment with continuous state spaces\"\n",
    "assert isinstance(env.action_space, gym.spaces.Discrete), \"This environment only works for environment with discrete action spaces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZES = [32]\n",
    "\n",
    "logits_net = mlp([obs_dim]+HIDDEN_SIZES+[act_dim])\n",
    "logits_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(obs):\n",
    "    logits  = logits_net(obs)\n",
    "    return Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs):\n",
    "    return get_policy(obs).sample().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(obs, acts, weights):\n",
    "    logp = get_policy(obs).log_prob(acts)\n",
    "    return -(logp * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([2]), torch.Size([100, 2]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_obs = torch.randn([4])\n",
    "batch_obs = torch.randn([100, 4])\n",
    "get_action(single_obs), logits_net(single_obs).shape, logits_net(batch_obs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: Categorical(logits: torch.Size([100, 2]))\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.6972, -1.2423, -1.3719, -1.0404, -0.8116, -0.5229, -0.6577, -1.0504,\n",
       "        -0.5490, -0.3167, -0.6885, -0.5156, -1.1858, -1.0370, -0.6696, -0.6612,\n",
       "        -0.5351, -0.7732, -1.0667, -0.3474, -0.7559, -0.3566, -0.5733, -0.5500,\n",
       "        -0.5365, -0.4914, -0.8499, -0.9789, -0.4449, -0.8379, -0.3769, -0.4798,\n",
       "        -0.8374, -0.9946, -1.0269, -0.7567, -0.6642, -0.7981, -0.4114, -0.7447,\n",
       "        -0.8401, -1.5441, -0.3796, -1.3311, -0.5071, -0.5817, -0.4597, -0.4234,\n",
       "        -0.5877, -1.0471, -0.7150, -0.6503, -0.6240, -0.2856, -0.6392, -0.7687,\n",
       "        -0.6203, -0.9354, -0.5417, -0.6070, -0.4574, -0.6183, -0.2483, -0.4218,\n",
       "        -1.5511, -0.2440, -0.7468, -0.4616, -0.4793, -0.3291, -0.6744, -0.6055,\n",
       "        -0.8404, -0.5995, -0.9508, -1.2535, -0.4451, -0.7578, -0.9332, -0.5868,\n",
       "        -0.5781, -0.8848, -0.7732, -0.3316, -0.3877, -0.3990, -0.9403, -0.4419,\n",
       "        -0.4145, -1.4540, -0.3879, -0.7687, -0.5717, -0.2877, -0.5896, -0.6645,\n",
       "        -0.4730, -0.2664, -0.5230, -1.5785], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = torch.randn(100, 4) # batch, obs_dim\n",
    "policy = get_policy(obs) # Categorical Policy - a distribution is returned\n",
    "\n",
    "print(\"Policy:\", policy)\n",
    "print(policy.sample())\n",
    "acts = torch.randint(0, 2, [100])\n",
    "policy.log_prob(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(env, optimizer, batch_size, render=True):\n",
    "    batch_obs = []\n",
    "    batch_acts = []\n",
    "    batch_weights = []\n",
    "    batch_rets = []\n",
    "    batch_lens = []\n",
    "    \n",
    "    # get initial observation from starting distribution\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_rews = []\n",
    "    \n",
    "    # Render the first episode of the epoch\n",
    "    finish_rendering_this_epoch = False\n",
    "    \n",
    "    while True:\n",
    "        # rendering\n",
    "        if (not finish_rendering_this_epoch) and render:\n",
    "            env.render()\n",
    "            # plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "            # display.display(plt.gcf())\n",
    "            # display.clear_output(wait=True)\n",
    "        \n",
    "        # save the current observation\n",
    "        batch_obs.append(obs.copy())\n",
    "        \n",
    "        # get action for the current observation\n",
    "        act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "        \n",
    "        # save the action and reward\n",
    "        ep_rews.append(rew)\n",
    "        batch_acts.append(act)\n",
    "        \n",
    "        if done:\n",
    "            # record info about the episode\n",
    "            ep_ret = sum(ep_rews)\n",
    "            ep_len = len(ep_rews)\n",
    "            \n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            batch_weights += [ep_ret] * ep_len # the weights are the returns for each episode, broadcasted to support\n",
    "                                                # the operation in function compute_loss\n",
    "            \n",
    "            #reset the environment\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            ep_rews = []\n",
    "            \n",
    "            # won't render again after first episode in the epoch\n",
    "            finish_rendering_this_epoch = True\n",
    "            \n",
    "            # end experience loop if we have enough of it\n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "        \n",
    "    # perform a single update step\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32), \n",
    "                              acts=torch.as_tensor(batch_acts, dtype=torch.float32),\n",
    "                              weights=torch.as_tensor(batch_weights, dtype=torch.float32))\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    env.close()\n",
    "\n",
    "    return batch_loss.item(), batch_rets, batch_lens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "\n",
    "optimizer = torch.optim.Adam(logits_net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 18.80 Average Return: 21.6 Average Steps: 21.6\n",
      "Epoch [2/50], Loss: 18.02 Average Return: 24.1 Average Steps: 24.1\n",
      "Epoch [3/50], Loss: 38.26 Average Return: 58.5 Average Steps: 58.5\n",
      "Epoch [4/50], Loss: 16.72 Average Return: 30.2 Average Steps: 30.2\n",
      "Epoch [5/50], Loss: 14.36 Average Return: 28.9 Average Steps: 28.9\n",
      "Epoch [6/50], Loss: 15.51 Average Return: 34.0 Average Steps: 34.0\n",
      "Epoch [7/50], Loss: 18.60 Average Return: 45.0 Average Steps: 45.0\n",
      "Epoch [8/50], Loss: 27.18 Average Return: 69.6 Average Steps: 69.6\n",
      "Epoch [9/50], Loss: 33.27 Average Return: 101.3 Average Steps: 101.3\n",
      "Epoch [10/50], Loss: 25.82 Average Return: 92.0 Average Steps: 92.0\n",
      "Epoch [11/50], Loss: 24.51 Average Return: 102.1 Average Steps: 102.1\n",
      "Epoch [12/50], Loss: 22.96 Average Return: 109.5 Average Steps: 109.5\n",
      "Epoch [13/50], Loss: 21.48 Average Return: 113.4 Average Steps: 113.4\n",
      "Epoch [14/50], Loss: 23.43 Average Return: 127.9 Average Steps: 127.9\n",
      "Epoch [15/50], Loss: 30.56 Average Return: 174.7 Average Steps: 174.7\n",
      "Epoch [16/50], Loss: 30.10 Average Return: 186.9 Average Steps: 186.9\n",
      "Epoch [17/50], Loss: 30.74 Average Return: 199.5 Average Steps: 199.5\n",
      "Epoch [18/50], Loss: 25.44 Average Return: 182.5 Average Steps: 182.5\n",
      "Epoch [19/50], Loss: 22.09 Average Return: 181.7 Average Steps: 181.7\n",
      "Epoch [20/50], Loss: 22.01 Average Return: 193.2 Average Steps: 193.2\n",
      "Epoch [21/50], Loss: 24.45 Average Return: 200.0 Average Steps: 200.0\n",
      "Epoch [22/50], Loss: 21.02 Average Return: 200.0 Average Steps: 200.0\n",
      "Epoch [23/50], Loss: 20.50 Average Return: 183.8 Average Steps: 183.8\n",
      "Epoch [24/50], Loss: 17.38 Average Return: 158.6 Average Steps: 158.6\n",
      "Epoch [25/50], Loss: 16.14 Average Return: 145.5 Average Steps: 145.5\n",
      "Epoch [26/50], Loss: 13.45 Average Return: 145.8 Average Steps: 145.8\n",
      "Epoch [27/50], Loss: 13.01 Average Return: 136.8 Average Steps: 136.8\n",
      "Epoch [28/50], Loss: 12.07 Average Return: 142.4 Average Steps: 142.4\n",
      "Epoch [29/50], Loss: 11.65 Average Return: 146.2 Average Steps: 146.2\n",
      "Epoch [30/50], Loss: 12.53 Average Return: 144.7 Average Steps: 144.7\n",
      "Epoch [31/50], Loss: 12.10 Average Return: 134.9 Average Steps: 134.9\n",
      "Epoch [32/50], Loss: 9.60 Average Return: 121.4 Average Steps: 121.4\n",
      "Epoch [33/50], Loss: 10.84 Average Return: 115.2 Average Steps: 115.2\n",
      "Epoch [34/50], Loss: 9.29 Average Return: 113.0 Average Steps: 113.0\n",
      "Epoch [35/50], Loss: 11.21 Average Return: 117.7 Average Steps: 117.7\n",
      "Epoch [36/50], Loss: 9.49 Average Return: 102.8 Average Steps: 102.8\n",
      "Epoch [37/50], Loss: 5.93 Average Return: 88.3 Average Steps: 88.3\n",
      "Epoch [38/50], Loss: 5.53 Average Return: 86.5 Average Steps: 86.5\n",
      "Epoch [39/50], Loss: 4.84 Average Return: 80.7 Average Steps: 80.7\n",
      "Epoch [40/50], Loss: 5.66 Average Return: 78.5 Average Steps: 78.5\n",
      "Epoch [41/50], Loss: 3.74 Average Return: 70.2 Average Steps: 70.2\n",
      "Epoch [42/50], Loss: 4.23 Average Return: 70.4 Average Steps: 70.4\n",
      "Epoch [43/50], Loss: 4.95 Average Return: 74.2 Average Steps: 74.2\n",
      "Epoch [44/50], Loss: 3.74 Average Return: 64.6 Average Steps: 64.6\n",
      "Epoch [45/50], Loss: 6.15 Average Return: 73.7 Average Steps: 73.7\n",
      "Epoch [46/50], Loss: 3.74 Average Return: 67.9 Average Steps: 67.9\n",
      "Epoch [47/50], Loss: 4.91 Average Return: 69.3 Average Steps: 69.3\n",
      "Epoch [48/50], Loss: 8.53 Average Return: 78.6 Average Steps: 78.6\n",
      "Epoch [49/50], Loss: 12.83 Average Return: 91.9 Average Steps: 91.9\n",
      "Epoch [50/50], Loss: 9.02 Average Return: 83.4 Average Steps: 83.4\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZES = 5_000\n",
    "EPOCHS = 50\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_loss, batch_rets, batch_lens = train_one_epoch(env, optimizer, BATCH_SIZES, render=True)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {batch_loss:.2f} Average Return: {np.mean(batch_rets):.1f} Average Steps: {np.mean(batch_lens):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7dfebdb0d38b25502d3e903ebcd5598394fdc0b22308289bf943c5905d42435"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
